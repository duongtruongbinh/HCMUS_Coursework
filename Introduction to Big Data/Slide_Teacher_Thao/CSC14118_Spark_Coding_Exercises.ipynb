{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epfrvqOy0XbK"
      },
      "source": [
        "# Install Java and Spark on Hadoop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pf3zOXQkRf0l",
        "outputId": "1831b3d5-74ce-4bfe-869c-5586dced7da0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "\r0% [Waiting for headers] [Waiting for headers] [Waiting for headers] [Connecting to ppa.launchpadcon\r                                                                                                    \rHit:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "\r0% [Waiting for headers] [Waiting for headers] [Connected to ppa.launchpadcontent.net (185.125.190.8\r                                                                                                    \rHit:3 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:7 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n"
          ]
        }
      ],
      "source": [
        "# install java\n",
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "# install spark (change the version number if needed)\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.5.1/spark-3.5.1-bin-hadoop3.tgz\n",
        "# unzip the spark file to the current folder\n",
        "!tar xf spark-3.5.1-bin-hadoop3.tgz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Llc1FhGNQ3U5"
      },
      "outputs": [],
      "source": [
        "# set your spark folder to your system path environment.\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.1-bin-hadoop3\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfAmwnpt1G5p"
      },
      "source": [
        "# Create a SparkSession in Python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ExFt_N8-z2m",
        "outputId": "fa7db96a-5453-4dad-d666-62e4caa3f604"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: findspark in /usr/local/lib/python3.10/dist-packages (2.0.1)\n"
          ]
        }
      ],
      "source": [
        "# start pyspark\n",
        "!pip install findspark\n",
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZU-oJLNVQl45"
      },
      "outputs": [],
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local\")\\\n",
        "          .appName(\"Spark APIs Exercises\")\\\n",
        "          .config(\"spark.some.config.option\", \"some-value\")\\\n",
        "          .getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chdGpJrATCDO"
      },
      "source": [
        "# Example 1: WordCount with Spark DataFrames and Spark RDDs\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91A6eRPJUl_0",
        "outputId": "649ac7c8-ad4a-43f2-af3d-75e9d3db66bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'CSC14118'...\n",
            "remote: Enumerating objects: 17, done.\u001b[K\n",
            "remote: Counting objects: 100% (17/17), done.\u001b[K\n",
            "remote: Compressing objects: 100% (15/15), done.\u001b[K\n",
            "remote: Total 17 (delta 1), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (17/17), 818.44 KiB | 3.99 MiB/s, done.\n",
            "Resolving deltas: 100% (1/1), done.\n"
          ]
        }
      ],
      "source": [
        "# Load the data\n",
        "!git clone https://github.com/nnthaofit/CSC14118.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJj_K5siTgAx"
      },
      "source": [
        "### Spark DataFrame-based WordCount"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NACaHF44TG_L",
        "outputId": "c92efa30-a9c9-4279-81b6-318acb5b5464"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------------------+\n",
            "|value                       |\n",
            "+----------------------------+\n",
            "|ppap                        |\n",
            "|i have a pen                |\n",
            "|i have an apple             |\n",
            "|ah apple pen                |\n",
            "|i have a pen                |\n",
            "|i have a pineapple          |\n",
            "|ah pineapple pen            |\n",
            "|ppap pen pineapple apple pen|\n",
            "+----------------------------+\n",
            "\n",
            "+---------+-----+\n",
            "|     word|count|\n",
            "+---------+-----+\n",
            "|      pen|    6|\n",
            "|     have|    4|\n",
            "|        i|    4|\n",
            "|    apple|    3|\n",
            "|pineapple|    3|\n",
            "|        a|    3|\n",
            "|     ppap|    2|\n",
            "|       ah|    2|\n",
            "|       an|    1|\n",
            "+---------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "linesDF = spark.read.text(\"CSC14118/ppap.txt\")\n",
        "linesDF.show(linesDF.count(),truncate = False)\n",
        "\n",
        "from pyspark.sql import functions as f\n",
        "wordsDF = linesDF.withColumn(\"word\", f.explode(f.split(f.col(\"value\"), \" \")))\\\n",
        "    .groupBy(\"word\")\\\n",
        "    .count()\\\n",
        "    .sort(\"count\", ascending = False)\n",
        "wordsDF.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82hKvCdcTj6g"
      },
      "source": [
        "###RDD-based WordCount"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ONOtAnLxSxYK",
        "outputId": "2c658068-0341-42f7-9400-dae1db6b77c5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('pen', 6),\n",
              " ('i', 4),\n",
              " ('have', 4),\n",
              " ('a', 3),\n",
              " ('apple', 3),\n",
              " ('pineapple', 3),\n",
              " ('ppap', 2),\n",
              " ('ah', 2),\n",
              " ('an', 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "linesRdd = spark.sparkContext.textFile(\"CSC14118/ppap.txt\")\n",
        "wordsRdd = linesRdd.flatMap(lambda line: line.split(\" \")) \\\n",
        "    .map(lambda word: (word, 1)) \\\n",
        "    .reduceByKey(lambda a, b: a + b)\\\n",
        "    .sortBy(lambda pair:-1*pair[1])\n",
        "wordsRdd.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 1: Data query with Spark DataFrame"
      ],
      "metadata": {
        "id": "eskNBM_J66UP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# clone the example data files from GitHub to Drive\n",
        "!git clone https://github.com/nnthaofit/CSC14118.git"
      ],
      "metadata": {
        "id": "T5YNLLU6656Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42ab0f89-4015-469a-e0e3-f3e6f9ba9a78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'CSC14118' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###0. Load the data file: movies.json"
      ],
      "metadata": {
        "id": "EbUxY_c-69xG"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZMLl4NC4t6_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1a. Show the schema of DataFrame that stores the movies dataset."
      ],
      "metadata": {
        "id": "HNfJKss_7cG1"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rJhoYcqQt8XV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1b. Show the number of distinct movies in the dataset"
      ],
      "metadata": {
        "id": "alyy1VZT7rve"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u6Irt5YPt955"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Count the number of movies released during the years 2012 and 2015 (included)"
      ],
      "metadata": {
        "id": "lbRNx-QU73ZN"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AQFTKSdYt_9G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Show the year in which the number of movies released is highest. One highest year is enough"
      ],
      "metadata": {
        "id": "Wm7NfqXp76Cy"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H6fj91IzuBcO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Show the list of movies such that for each film, the number of actors/actresses is at least five, and the number of genres it belongs to is at most two genres."
      ],
      "metadata": {
        "id": "EpfIvw-h8Eep"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_TD58zTjuEzm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Show the **movies** whose names are longest"
      ],
      "metadata": {
        "id": "TG1zzyVb8ND9"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YsBt1ARluGJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Show the movies whose name contains the word “fighting” (case-insensitive)."
      ],
      "metadata": {
        "id": "h-UJvT_I8QHF"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fvTK-g-AuHnu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Show the list of distinct genres appearing in the dataset"
      ],
      "metadata": {
        "id": "sZwOdpkG8Wkl"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e2Bkyz_DuI3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. List all movies in which the actor Harrison Ford has participated."
      ],
      "metadata": {
        "id": "zIS1IRFP8ZVt"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ooe3eFjDuJ8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. List all movies in which the actors/actresses whose names include the word “Lewis“ (case-insensitive) have participated."
      ],
      "metadata": {
        "id": "Gj6gxLvh8cXW"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GW8U4Yv1uLGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10. Show top five actors/actresses that have participated in most movies."
      ],
      "metadata": {
        "id": "s_EhHiV38fXV"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8iHf9EAluOMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exercise 2: RDD-based mainpulation\n",
        "\n",
        "\n",
        "*   The data is already in one ore more RDDs.\n",
        "*   You must not convert RDD to DF or use pure Python code.\n"
      ],
      "metadata": {
        "id": "aHQb93lRHj3u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Consider a string s that includes only alphabetical letters and spaces. Check whether s is a palindrome (case-insensitive)."
      ],
      "metadata": {
        "id": "o1F4O-msHoDj"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sTSEfW6WuQ_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Consider a string s that includes only alphabetical letters and spaces. Check whether s is a pangram (case-insensitive)."
      ],
      "metadata": {
        "id": "J6QVU6UWHwYl"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KT8sz6JduSkB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exercise 3: Frequent patterns and association rules mining"
      ],
      "metadata": {
        "id": "QOoFk6Z2H13m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 0. Load the data file: foodmart.csv\n",
        "\n",
        "\n",
        "*  A record is a tuple of binary values {0, 1}, each of which denotes the presence of an item (1: bought, 0: not bought).\n",
        "\n"
      ],
      "metadata": {
        "id": "uH3ozoTVH6J1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/nnthaofit/CSC14118.git\n",
        "df = spark.read.csv(\"CSC14118/foodmart.csv\", header=True, inferSchema = True)\n",
        "df.show()"
      ],
      "metadata": {
        "id": "4N9eo13QH_Tt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Convert the given data to the format required by Spark MLlib FPGrowth."
      ],
      "metadata": {
        "id": "FhDrkKXZH_dA"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SryD0vQOuek5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2.\tApply Spark MLlib FPGrowth to the formatted data. Mine the set of frequent patterns with the minimum support of 0.1. Mine the set of association rules with the minimum confidence of 0.9."
      ],
      "metadata": {
        "id": "OgVP1hS6IDVA"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BaAIAGDeugcy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exercise 4: Classification"
      ],
      "metadata": {
        "id": "pxSuXFXoIHO8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###0. Load the data file: mushroom.csv\n",
        "*   The data represents a collection of mushroom species.\n",
        "*   There are 8124 examples, each of which has 22 attributes and it is categorized into either “edible” (e) or “poisonous” (p)\n"
      ],
      "metadata": {
        "id": "mCMfeBzgIL0p"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SXGKh8EPIUJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.\tPrepare the train and test sets following the ratio 8:2"
      ],
      "metadata": {
        "id": "JdIdZJ2zIUge"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kAcyo8DJIZou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Fit a decision tree model on the training set, using Spark MLlib DecisionTreeClassifier with default parameters"
      ],
      "metadata": {
        "id": "zbshjQjnIZz8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TNbvDsqIIbNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Fit a random forest model on the training set, using Spark MLlib RandomForestClassification with default parameters"
      ],
      "metadata": {
        "id": "ghWaZW7LIbr-"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jVhzgPw7IcOG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Evaluate the two models on the same test set using the following metrics: areaUnderROC and areaUnderPR"
      ],
      "metadata": {
        "id": "TCWbqsk0IoSe"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G4YrCL5-IohK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###5. Chain the above steps into a single pipeline"
      ],
      "metadata": {
        "id": "cTJAVAGeIr3a"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I3wOtZdWIzsF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 5: Clustering"
      ],
      "metadata": {
        "id": "dn-fgVNlIxZN"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nqGZvNcWIxih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.\tCluster the data by using Spark MLlib KMeans with k = 2, 3, and 5, using Euclidean distance and cosine distance"
      ],
      "metadata": {
        "id": "IdHBhBHqIxqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DJV4mX3_IxyB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Evaluate each of the above clustering results using silhoutte score. Which configuration yeilds the best clustering?"
      ],
      "metadata": {
        "id": "dBo8pvyOIx5p"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3NekP_ioIyCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3. Chain the above steps into a single pipeline"
      ],
      "metadata": {
        "id": "GMAMYR1mJDk8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "429A9QeCIsT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###4. For each clustering result obtained above, count the number of examples that belong to each of the three species."
      ],
      "metadata": {
        "id": "h1ByWgvaJHp-"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AYyWW4ltJII8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 6: Network manipulation with Spark GraphFrames"
      ],
      "metadata": {
        "id": "T5epMoIlJOr1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###0. Load the data files: users.txt and followers.txt"
      ],
      "metadata": {
        "id": "I1pTLkFgJUmB"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q1WSkukOJT2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1.\tConstruct a graph from the given data to demonstrate a tiny social network\n"
      ],
      "metadata": {
        "id": "zd6XAz-aJYlj"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y7lOQereJONE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2.\tApply Graphs graphPageRank to the network to obtain a ranking list of users in terms of followers"
      ],
      "metadata": {
        "id": "hSV29hkuJct7"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SNAmL2y3Jc3E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3. Find connected components on the graph, using Graphs connectedComponents or stronglyConnectedComponents"
      ],
      "metadata": {
        "id": "nIciqOCqJeqH"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TqThg_PxJe0v"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}